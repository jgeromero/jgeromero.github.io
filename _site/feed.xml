<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-11-26T22:32:48-05:00</updated><id>http://localhost:4000/</id><title type="html"> >HiddenRoute </title><subtitle>Networking rants and findings, by jgeromero</subtitle><entry><title type="html">Highlights from NXTWORK 2018</title><link href="http://localhost:4000/2018/10/13/NXTWORK.html" rel="alternate" type="text/html" title="Highlights from NXTWORK 2018" /><published>2018-10-13T01:12:21-04:00</published><updated>2018-10-13T01:12:21-04:00</updated><id>http://localhost:4000/2018/10/13/NXTWORK</id><content type="html" xml:base="http://localhost:4000/2018/10/13/NXTWORK.html">&lt;h2&gt;Highlights from NXTWORK 2018&lt;/h2&gt;

&lt;p&gt;A few highlights I wanted to share from this year’s Juniper Networks customer and partner summit in Las Vegas:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This year’s theme is “Engineering Simplicity” which revolves around the idea that complexity is a hard challenge to solve for customers. They believe in developing products to simplify the management of different environments across on-prem private clouds and public clouds is key.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/Engineering-Simplicity.jpeg&quot; alt=&quot;Engineering-Simplicity&quot; title=&quot;Engineering Simplicity&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Different eras of networking, from the scale-up era (1990s), scale out era (2000s) and the era of multi-cloud where we are now.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/Eras-of-Networking.jpeg&quot; alt=&quot;Eras-of-Networking&quot; title=&quot;Eras of Networking&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Juniper believes the journey through those eras to be divided into 5 steps, from human driven (CLI) all the way to self driven networks. For them the industry right now is between step 2 and step 3 (building “intent” into networks and pushing workflows through automation tools, such as space or ansible).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/multicloud-journey.jpeg&quot; alt=&quot;multicloud-journey&quot; title=&quot;Multicloud Journey&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-cloud was a big topic this year and how challenging the management across both public and private clouds is becoming burdensome to customers. They think the answer to this is their new platform called Contrail Enterprise Multi-cloud.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A partnership with Nutanix was announced, going back to their theme of simplicity, pairing this up with contrail will allow teams to focus on intent and processes leaving the manual work to the underlying technologies.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt; Tools or other things I found interesting: &lt;/h3&gt;

&lt;p&gt;Panasonic RSU network&lt;/p&gt;

&lt;p&gt;Corero (DDoS Mitigation Platform)&lt;/p&gt;

&lt;p&gt;Juniper Sky (Think of space but SaaS)&lt;/p&gt;

&lt;p&gt;cSRX (containerized version of SRX)&lt;/p&gt;</content><author><name></name></author><category term="juniper" /><summary type="html">Highlights from NXTWORK 2018</summary></entry><entry><title type="html">Google Cloud Platform VPN load balancing</title><link href="http://localhost:4000/2018/04/29/GCP-VPN-load-balancing.html" rel="alternate" type="text/html" title="Google Cloud Platform VPN load balancing" /><published>2018-04-29T01:19:55-04:00</published><updated>2018-04-29T01:19:55-04:00</updated><id>http://localhost:4000/2018/04/29/GCP-VPN-load-balancing</id><content type="html" xml:base="http://localhost:4000/2018/04/29/GCP-VPN-load-balancing.html">&lt;h2&gt;Google Cloud Platform VPN Load balancing&lt;/h2&gt;

&lt;p&gt;Recently I worked on a project to build and design a fault-tolerant network with multiple VPNs load balancing traffic in order to increase the aggregate vpn throughput from HQ to a virtual private cloud (VPC) network.&lt;/p&gt;

&lt;p&gt;Cloud VPNs at GCP support up to a maximum of 1.5Gbps per tunnel over public internet. One of the solutions to increase VPN throughput is to set up multiple tunnels forwarding the same IP range at different IPs on your on-premise (HQ) VPN gateway. This will cause the cloud VPN gateway to automatically load balance between the configured tunnels.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This example assumes your on-premise VPN gateway’s throughput capabilities are supported.&lt;/p&gt;

&lt;p&gt;Our topology will look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/GCP-Tunnels.jpg&quot; alt=&quot;GCP-tunnels&quot; title=&quot;GCP Tunnels&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Total of 6 tunnels (3 in East1 region and 3 in East4) split between two cloud VPN Gateways.&lt;/li&gt;
  &lt;li&gt;Each of those tunnels will terminate in three different providers at our HQ SRX Cluster. In the event one goes down, there will be 2 other tunnels per region.&lt;/li&gt;
  &lt;li&gt;Cloud VPNs at GCP use ECMP (equal cost multi-path) to load balance traffic, we will use the same in our SRX cluster.&lt;/li&gt;
  &lt;li&gt;There will be two GCP network-tags that will be applied to our instances in East-1 and East-4 so they are properly routed out their corresponding VPN gateways forcing traffic to stay local to a given region.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt; Configuring Tunnels, Routes and Network Tags &lt;/h3&gt;

&lt;p&gt;We will start by having two VPN gateways, one in East-1 and another in East-4. Once we do that, we build our six tunnels forwarding the same subnet (in our case 10.0.0.0/8) to our three different peer IPs (provider-1, provider-2 and provider-3).&lt;/p&gt;

&lt;p&gt;In the end it should look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/Tunnels.jpg&quot; alt=&quot;tunnels&quot; title=&quot;tunnels&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now let’s create our VPC routes, we will forward 10.0.0.0/8 with our next-hop being the tunnels we created. We will also apply these routes to any instance that have the network-tags &lt;code class=&quot;highlighter-rouge&quot;&gt;routes-us-east1&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;routes-us-east4&lt;/code&gt; this is so VPN traffic for east1 and east4 egress through their corresponding cloud VPN gateways local to their region. A note about the routes above, since we use three routes per region pointing to our tunnels, this could probably have been done with a single route. However, I was not able to confirm if load balancing will take place by pointing it to a single tunnel as my next-hop.&lt;/p&gt;

&lt;p&gt;Our routes should look like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/routes.jpg&quot; alt=&quot;routes&quot; title=&quot;routes&quot; /&gt;&lt;/p&gt;

&lt;h3&gt; Configuring our SRX &lt;/h3&gt;

&lt;p&gt;As mentioned earlier, since we have multiple next-hop addresses (our tunnel interfaces) for the same destination (east1 and east4) with equal cost, Equal-cost multipath (ECMP) is a perfect candidate to use for this particular scenario.&lt;/p&gt;

&lt;p&gt;After configuring our VPN tunnels, we should’ve ended up with the following tunnel interfaces: &lt;code class=&quot;highlighter-rouge&quot;&gt;st0.40, st0.41, st0.42, st0.43, st0.44 and st0.45&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We then add the following static routes:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[edit routing-options static]
+    route 10.142.0.0/20 next-hop [ st0.40 st0.41 st0.42 ];
+    route 10.150.0.0/20 next-hop [ st0.43 st0.44 st0.45 ];
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Configure ECMP for our GCP routes:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[edit routing-options]
+   forwarding-table {
+       export GCP-load-balancing-policy;
+   }
[edit policy-options]
+   policy-statement GCP-load-balancing-policy {
+       from {
+           route-filter 10.142.0.0/20 orlonger;
+           route-filter 10.150.0.0/20 orlonger;
+       }
+       then {
+           load-balance per-packet;
+       }
+   }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Commit your configuration, configure your instances to use the network tags set up and traffic should start to load balance nicely across all your tunnels, here’s a snapshot of our network graphs:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/grafana-dash.jpg&quot; alt=&quot;dash&quot; title=&quot;dash&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><category term="other" /><summary type="html">Google Cloud Platform VPN Load balancing</summary></entry><entry><title type="html">Troubleshooting Network Slowness</title><link href="http://localhost:4000/2018/04/01/troubleshooting-network-slowness.html" rel="alternate" type="text/html" title="Troubleshooting Network Slowness" /><published>2018-04-01T01:18:30-04:00</published><updated>2018-04-01T01:18:30-04:00</updated><id>http://localhost:4000/2018/04/01/troubleshooting-network-slowness</id><content type="html" xml:base="http://localhost:4000/2018/04/01/troubleshooting-network-slowness.html">&lt;h2&gt;Troubleshooting Network Slowness&lt;/h2&gt;

&lt;h3&gt;First, let's go over some basics:&lt;/h3&gt;

&lt;p&gt;Bandwidth vs Throughput&lt;/p&gt;

&lt;p&gt;Bandwidth = How wide your channel is. Maximum amount of data that can travel through a channel.
Throughput = How much data actually travels through the channel successfully. This can be limited by tons of different things.&lt;/p&gt;

&lt;h3&gt;Things that may affect throughput:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Latency&lt;/li&gt;
  &lt;li&gt;Packet loss&lt;/li&gt;
  &lt;li&gt;Network Congestion&lt;/li&gt;
  &lt;li&gt;Small TCP Receive Windows&lt;/li&gt;
  &lt;li&gt;Traffic shaping and Rate limiting&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt; Long Fat Networks (LFN): &lt;/h3&gt;

&lt;p&gt;If you’re troubleshooting slowness on a &lt;strong&gt;High Bandwidth, High Delay&lt;/strong&gt; link, this applies to you. The idea behind it is you have a maximum (outstanding) amount of data on the wire on the way to the destination, while the sender is sitting idle waiting for acknowledgements for the data sent. This stop and wait is a waste of bandwidth, since the link is not being fully utilized.&lt;/p&gt;

&lt;p&gt;There’s a concept of &lt;strong&gt;Bandwidth Delay Product (BDP)&lt;/strong&gt;, which is basically how much outstanding data (not acknowledged) you can ever have on a wire for a given time assuming perfect network conditions. A link is considered a LFN when BDP exceeds 10^5 bits (12500 bytes). If your Receive Window is smaller than your BDP (often the case with LFNs) then you waste precious bandwidth. The ideal case should be TCP windows fully opening to what the BDP is for that link.&lt;/p&gt;

&lt;p&gt;BDP is calculated by taking the maximum data link capacity and multiplying it by the round trip delay &lt;strong&gt;(BxD)&lt;/strong&gt;, the results can be given in bits or bytes. LFN have high Bandwidth Delay Products because the amount of outstanding data on the wire lasts for longer time than those with lower BDPs. Therefore, TCP receive windows (max allowed unacknowledged data) negotiated at the beginning of sessions are stressed more often than those lighter not so fat networks. Remember: BDPs for a given link are assuming perfect network conditions.&lt;/p&gt;

&lt;p&gt;As a solution to increasing throughput on LFNs, you either increase the TCP receive windows past 65,535 bytes or lower the delay RTT. Window scaling was created for this reason (rfc7323).&lt;/p&gt;

&lt;p&gt;However, here is where &lt;strong&gt;Congestion Control&lt;/strong&gt; comes in and &lt;strong&gt;Congestion Windows&lt;/strong&gt;. Even if your windows are large enough on the receiver side but your network conditions are not ideal (Congestion, Packet loss etc) your congestion window (which is dynamically adjusted by the protocol) will never fully open, causing you to get much lower throughput.&lt;/p&gt;

&lt;p&gt;Here are two pictures showing what congestion windows look like during a TCP session:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/cwnd-and-rwnd-glossary.gif&quot; alt=&quot;CWND-image&quot; title=&quot;cwn-and-rwnd&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/cwnd-1024x203.png&quot; alt=&quot;CWND-image-2&quot; title=&quot;CWND&quot; /&gt;&lt;/p&gt;

&lt;h3&gt; Things to keep in mind &lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Congestion Windows are dynamically adjusted throughout the session and are greatly affected/limited by how well and healthy the condition of the network is. (Read more on TCP slow start, Slow Start Thresholds for more detail on congestion control)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Receive Windows are dynamically adjusted throughout session, with every ack the sender gets an updated window size, in a healthy network, the congestion window will quickly grow to the maximum window size negotiated. Limitations being max receive window sizes hard set at a number with no window scaling or application not picking up data fast enough from receive buffer. In any case, &lt;strong&gt;the sender is always bound by the cwnd&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt; Finding root cause &lt;/h3&gt;

&lt;p&gt;Use IPERF3 (leaner code base than iperf2) to test your throughput and see what your CWND looks like.&lt;/p&gt;

&lt;p&gt;Perform tcpdumps while your iperf3 tests are running (Make sure to start your captures before firing your iperf test to capture 3 way handshake so you get proper window sizes negotiated at the beginning of session).&lt;/p&gt;

&lt;p&gt;Load pcap caputure into wireshark and create IO graphs using the following filters (inspired by this &lt;a href=&quot;https://notalwaysthenetwork.com/2014/04/09/troubleshooting-with-wireshark-io-graphs-part-1/&quot;&gt;blog&lt;/a&gt; post):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;tcp.analysis.lost_segment&lt;/strong&gt; – Indicates we’ve seen a gap in sequence numbers in the capture.  Packet loss can lead to duplicate ACKs, which leads to retransmissions&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;tcp.analysis.duplicate_ack&lt;/strong&gt; – displays packets that were acknowledged more than one time.  A high number of duplicate ACKs is a sign of possible high latency between TCP endpoints&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;tcp.analysis.retransmission&lt;/strong&gt; – Displays all retransmissions in the capture.  A few retransmissions are OK, excessive retransmissions are bad. This usually shows up as slow application performance and/or packet loss to the user&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;tcp.analysis.window_update&lt;/strong&gt; – this will graph the size of the TCP window throughout your transfer.  If you see this window size drop down to zero(or near zero) during your transfer it means the sender has backed off and is waiting for the receiver to acknowledge all of the data already sent.  This would indicate the receiving end is overwhelmed.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;tcp.analysis.bytes_in_flight&lt;/strong&gt; – the number of unacknowledged bytes on the wire at a point in time.  The number of unacknowledged bytes should never exceed your TCP window size (defined in the initial 3 way TCP handshake) and to maximize your throughput you want to get as close as possible to the TCP window size.  If you see a number consistently lower than your TCP window size, it could indicate packet loss or some other issue along the path preventing you from maximizing throughput.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;tcp.analysis.ack_rtt&lt;/strong&gt; – measures the time delta between capturing a TCP packet and the corresponding ACK for that packet. If this time is long it could indicate some type of delay in the network (packet loss, congestion, etc)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;TCP is a very complex protocol, but hopefully this post has provided some pointers to pinpoint slowness on your links.&lt;/p&gt;</content><author><name></name></author><category term="other" /><summary type="html">Troubleshooting Network Slowness</summary></entry><entry><title type="html">Policy Statments for dynamic IGPs</title><link href="http://localhost:4000/2018/02/18/policy-statements-for-dynamic-IGPs.html" rel="alternate" type="text/html" title="Policy Statments for dynamic IGPs" /><published>2018-02-18T00:16:00-05:00</published><updated>2018-02-18T00:16:00-05:00</updated><id>http://localhost:4000/2018/02/18/policy-statements-for-dynamic-IGPs</id><content type="html" xml:base="http://localhost:4000/2018/02/18/policy-statements-for-dynamic-IGPs.html">&lt;h2&gt;Policy Statements for dynamic IGPs&lt;/h2&gt;

&lt;p&gt;I thought about writing about this policy-statement I came across the other day:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;term 10 {
    from {
        protocol [ static direct ];
        route-filter 10.50.0.0/16 orlonger;
    }
    then accept;
}
term 100 {
    then reject;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This policy statement is used to export (inject) routes into IS-IS. The key thing to remember here is the route filter applies to anything in the routing table that comes from static or direct routes:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jromero@mx-480&amp;gt; show route protocol static

And

jromero@mx-480&amp;gt; show route protocol direct

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Keep in mind that if you match &lt;code class=&quot;highlighter-rouge&quot;&gt;exact&lt;/code&gt; prefixes in your route-filter then your routing table should have an active aggregate route:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jromero@mx-480&amp;gt; show route protocol aggregate

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><category term="juniper" /><summary type="html">Policy Statements for dynamic IGPs</summary></entry><entry><title type="html">Welcome!</title><link href="http://localhost:4000/2018/01/26/welcome.html" rel="alternate" type="text/html" title="Welcome!" /><published>2018-01-26T20:53:34-05:00</published><updated>2018-01-26T20:53:34-05:00</updated><id>http://localhost:4000/2018/01/26/welcome</id><content type="html" xml:base="http://localhost:4000/2018/01/26/welcome.html">&lt;h2&gt;Welcome!&lt;/h2&gt;

&lt;p&gt;Hi all, this blog will serve as a place holder for notes and share some of my experiences. Thank you for visiting.&lt;/p&gt;</content><author><name></name></author><category term="other" /><summary type="html">Welcome!</summary></entry></feed>
